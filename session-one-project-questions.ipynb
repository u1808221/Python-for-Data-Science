{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science Practicce Session 1: Social Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Coronavirus Affecting Lives Across the World?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [COVIDiSTRESS global survey](https://osf.io/z39us/) is an international collaborative undertaking for gathering data on human experiences, behaviour and attitudes during the COVID-19 pandemic. The survey focuses on psychological stress, compliance with behavioural guidelines to slow the spread of Coronavirus, and trust in governmental institutions and their preventive measures. \n",
    "\n",
    "In this notebook we are going to use the results of the COVIDiSTRESS survey conducted in Aprilâ€“May 2020 to investigate the impact of the Coronavirus pandemic on the perceived level of stress by individuals across the world as well as explore other related factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started we need to import the `pandas` packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the relevant dataset as a CSV file from this session's [materials](https://education.wdss.io/python-for-data-science/session-one/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is more complex than the one we looked at in the teaching session and so we will need to acquaint ourselves with some more optional arguments of the `read_csv` function. Namely, look at the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) of the function, to see what the following parameters are for:\n",
    "\n",
    "- `low_memory`\n",
    "- `parse_dates`\n",
    "- `dayfirst`\n",
    "- `index_col`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this knowledge to import the dataset and assign it to the variable `raw_df`. Pay close attention to the following points:\n",
    "\n",
    "- The dataset has a date column `RecordedDate` which uses day first format\n",
    "- Because many columns have lots of missing values, there will likely be mixed type inference (a bad thing) when using low memory mode\n",
    "- The first column of the dataset consists of row numbers which should be read as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bonus**\n",
    ">\n",
    "> Suppose we didn't trust the index column of the original dataset. How could we use the default index provide by `pandas` instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset by doing the following:\n",
    "- Print the first 3 rows of the dataset\n",
    "- Print the dimensions of the dataset\n",
    "- Print numerical summaries of the columns\n",
    "- Print the data types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 3 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical summaries of the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bonus**\n",
    ">\n",
    "> When printing the above, we only get to see 10 columns in total. For printing out full datasets and series, checkout the [option context](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.option_context.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 152 columns in the dataset, many of which we won't need in our analysis. Instead, we want to focus on the following variables.\n",
    "\n",
    "| description | variable name | measurement |\n",
    "| --- | --- | --- |\n",
    "| date of record | `RecordedDate` | date \"DD/MM/YYYY hh:mm\"\n",
    "| age | `Dem_age` | age in years\n",
    "| gender | `Dem_gender` | Male/Female\n",
    "| country | `Country` | country name\n",
    "| employment status| `Dem_employment` | type of employment\n",
    "| perceived stress for the past week | `Scale_PSS10_UCLA_1` to `Scale_PSS10_UCLA_10` | 1=never, 5=very often; 10 items\n",
    "| trust in institutions | `OECD_insititutions_1` (government), `OECD_insititutions_4` (health system) | 0=not at all, 10=completely\n",
    "| trust in country's preventive measures | `Trust_countrymeasure` | 0=too little, 5= appropriate, 11=too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stress was meassured in 10 different categories. Create a list, `stress_columns` of column names `Scale_PSS10_UCLA_X`, where `X` ranges from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_columns = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then combine these with the other column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'RecordedDate', 'Dem_age', 'Dem_gender', 'Country', 'Dem_employment',\n",
    "    'OECD_insititutions_1', 'OECD_insititutions_4', 'Trust_countrymeasure'\n",
    "] + stress_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a subset of this dataset, `covid_stress`, containing only columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_stress = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset is fairly large so we may want to remove it from memory now that we no longer need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear up memory\n",
    "del raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many fields in the dataset have missing/unknown values. We can use `covid_stress.isna()` to return a Boolean dataset where each value is `True` if the original value in that field was missing. Let's investigate where these missing values are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember, `True`/`False` are converted to `1`/`0` respectively when cast to integers. This means we can use the `sum` and `mean` methods on a Boolean dataset or series to count or find proportions of `True`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values are there in each column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What proportion of records (rows) have at least one missing value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have 65535 records in total we can drop all the records with at least one missing value and still have many tens of thousands left. We will learn how to deal with missing values properly in later sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we use the `dropna` method of the dataframe. Search for \"pandas drop missing values\" on the web to find the documentation for this. Save the cleaned dataset as `covid_stress_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_stress_clean = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bonus**\n",
    ">\n",
    "> Recall from the teaching session that many pandas operations can be done _inplace_. How would we drop rows with missing values in this fashion (check the documentation again if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a random sample of 10 rows and verify that they have no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample of cleaned dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, the `covid_stress` dataset isn't too large so we don't need to worry about deleting it. Sometimes it's worth holding onto these things in case we need to go back to them and don't want to rerun cells. How big is the dataset exactly? Have a look at the documentation for the `info` method, which can answer this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we will make a copy of the current dataframe. This will make it easier to return to this point by rerunning this cell if any mistakes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_stress_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels of stress were measured in 10 different categories on a scale from 1=never, 5=very often. We want analyse only aggregated information about the stress level, i.e. the mean of those 10 values. For this, we create a new column `Avg_stress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column by averaging all stress columns\n",
    "\n",
    "\n",
    "# Drop the now redundant stress columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names `OECD_insititutions_1`, `OECD_insititutions_4` are not the most friendly. Let's rename them to something more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    \"OECD_insititutions_1\" : \"Trust_gov\",\n",
    "    \"OECD_insititutions_4\" : \"Trust_health\"\n",
    "}\n",
    "covid_df = covid_df.rename(mapper, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start answering questions about the dataset. Let's print out the data types of the columns to check that everything is as expected before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the variable types of each of the column is as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the study was conducted? (i.e. what is the range of dates in the records?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum dates in dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of observations are from Finland, Sweden, or Norway? Use `.round()` to print the answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Finish, Swedish, Norwegian respondents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dataset sorted by age in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by decreasing age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the median value for age? Print your answer as part of a meaningful sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median age of respondents\n",
    "print(\"The median age of respondents is\", covid_df.Dem_age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the gender breakdown of respondents? Use the `value_counts` method to find out (after searching for its documentation of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender breakdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nicer to see percentages rather than absolute counts. There are many ways to do this. Take your pick or write down as many as you can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender breakdown as proportions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to divide the respondents in 4 age of equal width, how many respondents would land in each group? (This can also be performed using `value_counts` too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binned value counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of the observations have each employment type, excluding students and retirees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment types for non-students/retirees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stress During the COVID-19 Pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform grouped summaries using the `groupby` method, documented [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html). These returned a grouped dataframe, which we can apply aggregation methods such as `mean` and `min` to, in which case the aggregation is applied separately to each group. An example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First observation for each gender\n",
    "covid_df.groupby('Dem_gender')['RecordedDate'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the 10 countries with the highest average value of stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much can we trust these values? Are there enough respondents from each of those countries to make an an inference about the level of stress of the entire country? It would be helpful to aggregate both the mean stress level and observation count. We can do this using the `agg` method. Read the documentation for this then use it to recreate the above result with counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprising? Now let's identify 10 countries with highest average value of stress with at least 200 respondents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df[country_df['Count'] >= 200] \\\n",
    "    .sort_values('Mean_stress', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend this comparison, we wish to create a dateset `country_fltr` which contains the mean stress, trust in health organisations, government and health measures, and the observation count for all countries with at least 200 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = covid_df.groupby('Country').agg(\n",
    "    Mean_stress=('Avg_stress', 'mean'),\n",
    "    Mean_gov_trust=('Trust_gov', 'mean'),\n",
    "    Mean_health_trust=('Trust_health', 'mean'),\n",
    "    Mean_measures_trust=('Trust_countrymeasure', 'mean'),\n",
    "    Count=('Country', 'count')\n",
    ")\n",
    "\n",
    "country_fltr = country_df[country_df.Count>200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sort `country_fltr` by different categories and in ascending/descending order, to explore these 4 measure across the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be interesting to compare the trust in country's health system with the average stress level. One way to do it is to make a scatter plot with the mean trust in health on the x-axis and mean level of stress on the y-axis. (More about plotting in next session, but I couldn't resist making at least one plot in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_fltr.plot(x='Mean_health_trust', y='Mean_stress', kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! It seems like there is a negative correlation between these two variables. Can you spot the outlier? Find out which country this datapoint corresponds to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Closer Look at the UK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will draw comparisons between the United Kingdom and the rest of Europe. We have provided an Excel spreadsheet in the [session materials](https://education.wdss.io/python-for-data-science/session-one/) that maps countries to continents, which can be used to aid in this comparison. Start by loading this dataset into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the country mapping spreadsheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas series of all European countries except the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this series to create two dataframes, `uk_df` and `euro_df` containing observations from the UK and rest of Europe respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = # ...\n",
    "euro_df = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the average stress level in the UK above or below average of the rest of Europe? By how much? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of people in the UK aged between 40 and 60 (inclusive) reported an average stress level above 3? This is a longer question so try to break it up into sensible chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the average stress level for each employment status and gender pair in Europe? (Note: you can use a list of labels when grouping a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wow! What happened there? We've just been introduced to the pandas multi-index, a relatively unique feature of the package which offers incredible power and flexibility. This is considered an advanced feature and so we will not go into any more detail about it but you can read more [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the trust in government tend to increase/decrease or remain stable over the period considered in the report? Compare the mean trust in government in first 3 weeks of April starting from 2020-04-06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this using base Python `for` loops. Loop through through consecutive pairs of days in the provided date list and calculate the mean trust in government for an subset of the dataset filtered to only have days between the current date in the list and 7 days in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't the most elegant and is certainly not the most efficient. A better solution is below, though it goes to show that pandas and base Python can be used together when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df.groupby(uk_df.RecordedDate.dt.isocalendar().week).mean()['Trust_gov']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Time to Shine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come up with some new questions about this dataset and try to answer them using your newly obtained skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python for Data Science",
   "language": "python",
   "name": "pyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
